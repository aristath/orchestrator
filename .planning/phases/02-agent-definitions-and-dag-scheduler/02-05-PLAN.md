---
phase: 02-agent-definitions-and-dag-scheduler
plan: 05
type: execute
wave: 4
depends_on: ["02-01", "02-02", "02-03", "02-04"]
files_modified:
  - internal/scheduler/integration_test.go
autonomous: true

must_haves:
  truths:
    - "Config loads defaults, creates backends from provider config, and feeds into executor"
    - "DAG validates, executor runs eligible tasks, workflow spawns follow-ups"
    - "End-to-end: config -> DAG -> execute -> workflow follow-up -> execute follow-up"
    - "File resource locks prevent concurrent writes during parallel-simulated execution"
  artifacts:
    - path: "internal/scheduler/integration_test.go"
      provides: "Integration tests validating all Phase 2 subsystems work together"
      contains: "func TestIntegration"
  key_links:
    - from: "internal/scheduler/integration_test.go"
      to: "internal/config/loader.go"
      via: "Loads config to drive test setup"
      pattern: "config\\.Load"
    - from: "internal/scheduler/integration_test.go"
      to: "internal/scheduler/executor.go"
      via: "Executes tasks through the full pipeline"
      pattern: "executor\\.ExecuteTask"
    - from: "internal/scheduler/integration_test.go"
      to: "internal/scheduler/workflow.go"
      via: "Spawns follow-up tasks after completion"
      pattern: "workflowMgr\\.OnTaskCompleted"
---

<objective>
Integration tests validating all Phase 2 subsystems work together end-to-end.

Purpose: Individual unit tests verify each component in isolation. This plan verifies they compose correctly: config loads providers/agents, DAG constructs and validates, executor runs tasks through mock backends with resource locking, and workflow manager spawns follow-ups that re-enter the DAG.

Output: Integration test file proving the full Phase 2 pipeline works.
</objective>

<execution_context>
@/Users/aristath/.claude/get-shit-done/workflows/execute-plan.md
@/Users/aristath/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-agent-definitions-and-dag-scheduler/02-01-SUMMARY.md
@.planning/phases/02-agent-definitions-and-dag-scheduler/02-02-SUMMARY.md
@.planning/phases/02-agent-definitions-and-dag-scheduler/02-03-SUMMARY.md
@.planning/phases/02-agent-definitions-and-dag-scheduler/02-04-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: End-to-end integration tests</name>
  <files>internal/scheduler/integration_test.go</files>
  <action>
Create `internal/scheduler/integration_test.go` with these integration scenarios:

**Test 1: Full pipeline (config -> DAG -> execute -> workflow)**

1. Load default config via `config.DefaultConfig()`
2. Create a DAG with 3 tasks:
   - "implement-auth" (AgentRole: "coder", no deps, writes: ["auth.go"])
   - "implement-db" (AgentRole: "coder", no deps, writes: ["db.go"])
   - "implement-api" (AgentRole: "coder", depends on both above, writes: ["api.go"])
3. Validate DAG -- should succeed, order should have auth and db before api
4. Create Executor with mock backends for "coder", "reviewer", "tester" roles
5. Create WorkflowManager with the default "standard" workflow (coder -> reviewer -> tester)
6. Get eligible tasks -- should be "implement-auth" and "implement-db"
7. Execute both eligible tasks (sequentially in test, but they COULD run in parallel)
8. After each completion, call WorkflowManager.OnTaskCompleted -- should spawn reviewer follow-ups
9. Get eligible again -- "implement-api" should now be eligible, plus the reviewer follow-ups
10. Execute "implement-api"
11. After completion, verify reviewer follow-ups exist in DAG
12. Execute a reviewer follow-up, verify tester follow-up is created
13. Verify final state: all original tasks completed, follow-up chain exists

**Test 2: Resource lock contention simulation**

1. Create DAG with 2 tasks that write the SAME file: "task-a" writes ["shared.go"], "task-b" writes ["shared.go"], no deps between them
2. Both are eligible (no dependencies)
3. Launch both ExecuteTask calls in separate goroutines
4. Use a mock backend with an artificial 50ms delay (time.Sleep)
5. Verify both complete (no deadlock)
6. Verify they did NOT run simultaneously (resource lock prevented it) -- use timestamps or a shared counter

**Test 3: Failure propagation**

1. Create DAG: "build" (FailHard) -> "deploy", "lint" (FailSoft) -> "test"
2. Execute "build" with a failing mock backend
3. Verify "deploy" is NOT eligible (hard failure blocks)
4. Execute "lint" with a failing mock backend
5. Verify "test" IS eligible (soft failure allows)

**Test 4: Config-driven backend selection**

1. Load a config with:
   - Provider "claude" (type "claude")
   - Provider "codex" (type "codex")
   - Agent "coder" using provider "claude"
   - Agent "reviewer" using provider "codex"
2. Verify that the config correctly maps agent roles to provider types
3. Create mock backends keyed by role, verify executor dispatches to correct backend per task's AgentRole

Use the same `mockBackend` pattern from executor_test.go (or share via a testutil if cleaner). Each mock should record which calls it received so assertions can verify correct routing.
  </action>
  <verify>
Run `cd /Users/aristath/orchestrator && go test ./internal/scheduler/ -run TestIntegration -v -count=1 -race -timeout=30s` -- all tests pass.
  </verify>
  <done>
4 integration tests validate: full pipeline (config->DAG->execute->workflow), resource lock contention, failure propagation (hard vs soft), and config-driven backend selection. All pass under -race with 30s timeout.
  </done>
</task>

<task type="auto">
  <name>Task 2: Full package test suite and go vet</name>
  <files></files>
  <action>
Run the complete test suite for both packages to ensure nothing is broken:

```bash
cd /Users/aristath/orchestrator && go test ./internal/config/ ./internal/scheduler/ -v -count=1 -race -timeout=60s
cd /Users/aristath/orchestrator && go vet ./internal/config/ ./internal/scheduler/ ./internal/backend/
```

If any tests fail, fix the issues. Common problems:
- Import cycles between config and scheduler (if any, break by using interfaces or moving types)
- Race conditions in integration tests (ensure proper synchronization)
- Flaky timing tests (use channels/mutexes instead of time.Sleep for synchronization where possible, keep time.Sleep only in the mock backend delay simulation)

Also verify the existing Phase 1 tests still pass:
```bash
cd /Users/aristath/orchestrator && go test ./internal/backend/ -v -count=1 -race
```

If Phase 1 tests break (they should not since we added new packages, not modified existing ones), investigate and fix.
  </action>
  <verify>
All three commands succeed: `go test ./internal/config/ ./internal/scheduler/ -race`, `go vet ./...`, and `go test ./internal/backend/ -race`.
  </verify>
  <done>
Full test suite passes for config, scheduler, and backend packages. go vet reports no issues across all packages. No regressions in Phase 1 code.
  </done>
</task>

</tasks>

<verification>
- `go test ./internal/config/ ./internal/scheduler/ -v -count=1 -race -timeout=60s` all pass
- `go test ./internal/backend/ -v -count=1 -race` Phase 1 tests still pass
- `go vet ./...` no issues across entire project
- Integration tests prove end-to-end pipeline works
</verification>

<success_criteria>
- Full pipeline integration test: config -> DAG -> execute -> workflow follow-up -> execute follow-up
- Resource lock contention test: concurrent tasks writing same file are serialized
- Failure propagation test: FailHard blocks, FailSoft allows
- Config-driven routing test: different agent roles dispatch to different backends
- All Phase 1 tests still pass (no regressions)
- go vet clean across all packages
</success_criteria>

<output>
After completion, create `.planning/phases/02-agent-definitions-and-dag-scheduler/02-05-SUMMARY.md`
</output>
